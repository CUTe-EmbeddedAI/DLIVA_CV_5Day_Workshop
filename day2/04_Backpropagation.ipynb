{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyk2sCJbgWsU"
      },
      "source": [
        "# Backpropagation\n",
        "\n",
        "Take a look at the mathematics of the backpropagation algorithm.\n",
        "\n",
        "We will cover the following:\n",
        "\n",
        "- Notations\n",
        "\n",
        "- Forward pass\n",
        "\n",
        "- Backward pass\n",
        "\n",
        "- The chain rule for the backward pass\n",
        "\n",
        "Neural Networks (NN) are non-linear classifiers that can be formulated as a series of matrix multiplications. Just like linear classifiers, they can be trained using the same principles we followed before, namely the gradient descent algorithm. The difficulty arises in computing the gradients. \n",
        "\n",
        "But first things first. \n",
        "\n",
        "Let’s start with a straightforward example of a two-layered NN, with each layer containing just one neuron."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAlRUAuLgWsd"
      },
      "source": [
        "# Notations\n",
        "\n",
        "- The superscript defines the layer that we are in.\n",
        "\n",
        "- $o^{L}$ denotes the activation of layer L.\n",
        "\n",
        "- $w^{L}$ is a scalar weight of the layer L.\n",
        "\n",
        "- $b^{L}$ is the bias term of layer L.\n",
        "\n",
        "- $C$ is the cost function, $t$ is our target class, and $f$ is the activation function.\n",
        "\n",
        "\n",
        "# Forward pass\n",
        "\n",
        "Our lovely model would look something like this in a simple sketch:\n",
        "\n",
        "\n",
        "![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig09.PNG)\n",
        "\n",
        "We can write the output of a neuron at layer $L$ as:\n",
        "\n",
        "> $o^{L} = f(w^{L}o^{L-1} + b^{L})$\n",
        "\n",
        "To simplify things, let’s define:\n",
        "\n",
        "> $z^{L} = w^{L}o^{L-1} + b^{L}$\n",
        "\n",
        "so that our basic equation will become:\n",
        "\n",
        "> $o^L = f(z^l)$\n",
        "\n",
        "We also know that our loss function is:\n",
        "\n",
        "> $C = (o^L − t)^2 $\n",
        "\n",
        "This is the so-called **forward pass**. \n",
        "\n",
        "We take some input and pass it through the network. From the output of the network, we can compute the loss $C$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFXCIW4ngWsf"
      },
      "source": [
        "# Backward pass\n",
        "\n",
        "> Backward pass is the process of adjusting the weights $w$ in all the layers to minimize the loss $C$.\n",
        "\n",
        "To adjust the weights based on the training example, we can use our known **update rule**:\n",
        "\n",
        "> $w_t^L = w_{t-1}^L - \\lambda \\frac{\\delta C}{\\delta w^L} $\n",
        "\n",
        "where $\\lambda$ is the learning rate that scales down the gradient.\n",
        "\n",
        "It should be clear by now that the only thing left to compute is the gradient $\\frac{\\delta C}{\\delta w^L}$ (the derivative of the loss with respect to the weight).\n",
        "\n",
        "One way to think about computing $\\frac{\\delta C}{\\delta w^L}$ is through the following diagram, which is called computational graph:\n",
        "\n",
        "\n",
        "![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig10.PNG)\n",
        "\n",
        "We summarize the performed operation in this way. To convert this into math, we need to revisit the chain rule."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzS3TKW8gWsg"
      },
      "source": [
        "# The chain rule for the backward pass\n",
        "\n",
        "To compute the gradient $\\frac{\\delta C}{\\delta w^L}$, our most useful tool is calculus and the chain rule.\n",
        "\n",
        "Using both, we can write:\n",
        "\\frac{\\delta o^L}{\\delta z^L \\frac{\\delta C}{\\delta o^L}}\n",
        "> $\\frac{\\delta C}{\\delta w^L} = \\frac{\\delta z^L}{\\delta w^L} \\frac{\\delta o^L}{\\delta z^L} \\frac{\\delta C}{\\delta o^L}$\n",
        "\n",
        "It is evident that the final gradient is affected by the gradients of the previous neuron, which in turn is affected by the gradients of the one before. You can see that in order to compute the gradient, we need to go back (through the chain rule) all the way to the beginning of the network.\n",
        "\n",
        "In other terms, we need to propagate the error backwards. This is how the backpropagation algorithm got its name.\n",
        "\n",
        "To find the gradients, let’s compute all the subgradients. By using basic calculus,we get:\n",
        "\n",
        ">> $C = (o^L - t)^2 \\rightarrow \\frac{\\delta C}{\\delta o^L} = 2(o^L - t)$\n",
        "\n",
        ">> $o^L = f(w^Lo^{L-1} + b^L) = f(z^L) \\rightarrow \\frac{\\delta o^L}{\\delta z^L} = f'(o^L)$\n",
        "\n",
        ">> $z^L = w^Lo^{L-1} + b^L \\rightarrow \\frac{\\delta z^L}{\\delta w^L} = o^{L-1}$\n",
        "\n",
        "Combining them all together, we acquire our final gradient:\n",
        "\n",
        ">> $ \\frac{\\delta C}{\\delta w^L} = o^{L-1} \\ast f'(o^L) \\ast 2(o^L - t)$\n",
        "\n",
        "Similar equations can be derived for the biases. Instead of $\\frac{\\delta z^L}{\\delta b^L}$, we would have:\n",
        "\n",
        ">> $ z^L = w^Lo^{L-1} + b^L \\rightarrow \\frac{\\delta z^L}{\\delta b^L} =1 $ \n",
        "\n",
        "For completion, if we do the math, we get:\n",
        "\n",
        ">> $\\frac{\\delta C}{\\delta b^L} = f'(o^L) \\ast 2(o^L - t)$\n",
        "\n",
        "\n",
        "Now, we can adjust the weight and biases based on a single training example based on the update rule:\n",
        "\n",
        ">> $w^L_t = w^L_{t-1} - \\lambda \\frac{\\delta C}{\\delta w^L}$\n",
        "\n",
        "Next, we’ll feed the next example and readjust, and repeat. This is the infamous **BACKPROPAGATION**.\n",
        "\n",
        "You might argue that this is oversimplified because we only have 1 neuron. To be honest, not much will change if we add more neurons per layer. We will essentially conclude to the same equation.\n",
        "\n",
        "\n",
        "![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig11.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDEP_y12gWsh"
      },
      "source": [
        "Two final things to note here:\n",
        "\n",
        "- The derivative with respect to the activation is a summation due to the fact that the activation of a neuron now depends on the activations of all th eneurons on the previous layer.\n",
        "\n",
        "- The same derivative also depends on the derivatives of the next layer’s activation (backpropagation of the error).\n",
        "\n",
        "You now have a sense of how NNs learn, and that is no easy task.\n",
        "\n",
        "> **Important note:** We will not be computing gradients in every network that we define. The gradients are computed automatically in modern frameworks such as PyTorch.\n",
        "\n",
        "No more partial derivatives!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "04. Backpropagation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}