{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF87xrmxfaSy"
      },
      "source": [
        "# Neural Networks\n",
        "\n",
        "Understand the neural network, how it is formulated, and why it works.\n",
        "\n",
        "We will cover the following:\n",
        "\n",
        "- What is a neuron?\n",
        "\n",
        "- Multilayer perceptron\n",
        "\n",
        "- Universal approximation theorem\n",
        "\n",
        "- Deep neural networks as feature extractors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8t7SBNKfaTB"
      },
      "source": [
        "# What is a neuron?\n",
        "\n",
        "In the world of Neural Networks (NN), the basic building block is the neuron. NNsare nothing more than a collection of neurons organized in layers, with information passing from one layer to the other. So to understand NN, we first need to understand the neuron: the basic computing unit.\n",
        "\n",
        "\n",
        "![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig06.PNG)\n",
        "Mathematically, we have:\n",
        "\n",
        "> $y = w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3}$\n",
        "\n",
        "A neuron is simply a linear classifier with a single output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0fwV8FPhfaTI"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn \n",
        "\n",
        "neuron = nn.Linear ( 3 , 1 , bias= False )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL3ACAMDfaTS"
      },
      "source": [
        "Looks familiar? In most applications, we also add a bias $b$ to shift the position of the boundary line that separates the data points. This is also popularly known as **Perceptron**.\n",
        "\n",
        "To extend this idea, we also pass this weighted average through a **non-linear function** $\\sigma$ that will give us the decision boundary.\n",
        "\n",
        "Why?\n",
        "\n",
        "Because with non-linear functions between linear layers, we can model muchmore complex representations with less linear layers.\n",
        "\n",
        "> Non-linearities is a key component that makes NN very rich function approximators.\n",
        "\n",
        "Putting it all together, we have:\n",
        "\n",
        "> $y = \\sigma(w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3} + b) = \\sigma(wx + b)$\n",
        "\n",
        "We organize neurons in layers with nn.Linear(in_features, out_features) and stack layers in sequential order. The layer’s stacking combined with the non-linear activation function gives us the ability to distinguish non-linearly separable data.\n",
        "\n",
        "There are multiple names in the literature for stacking linear layers with non-linear activations: **Multi-Layer Perceptron (MLP), artificial Neural Network, and feedforward module**. All these terms mean the same thing.\n",
        "\n",
        "In practice, for three input features and two classes, our model will be like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdgLhPlcfaTX",
        "outputId": "d5645052-7dde-4ae4-b14e-67b81c94c05b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=3, out_features=20, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=20, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn \n",
        "\n",
        "## 20 is the hidden dimension. arbitary choice \n",
        "model = nn.Sequential( \n",
        "        nn.Linear(3,20), # 3 for the input features x1,x2,x3 \n",
        "        nn.ReLU(), \n",
        "        nn.Linear(20,2)) # 2 for the classes \n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wv64d7ufaTa"
      },
      "source": [
        "Here, The activation function $\\sigma$ is nn.ReLU().\n",
        "\n",
        "\n",
        "![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig07.PNG)\n",
        "\n",
        "A good practice for you is to write the depicted image in Pytorch.\n",
        "\n",
        "> **Hint:** It is not that different from the illustrated code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MnMMk9QfaTc"
      },
      "source": [
        "# Multilayer Perceptron\n",
        "\n",
        "But why are MLPs able to find non-linear functions? Let’s have another look at NNs from a mathematical perspective. Remember that each neuron is represented with:\n",
        "\n",
        "> $y = \\sigma(w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3} + b) = \\sigma(wx + b)$\n",
        "\n",
        "So for two neurons nn.Linear(3,2), we would have:\n",
        "\n",
        "> $y_{1} = \\sigma(w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3} + b) = \\sigma(wx + b)$\n",
        "\n",
        "> $y_{2} = \\sigma(w_{4}x_{1} + w_{5}x_{2} + w_{6}x_{3} + b) = \\sigma(wx + b)$\n",
        "\n",
        "This can be transformed using linear algebra to $y = \\sigma(Wx + b)$ where:\n",
        "\n",
        "> $y = \\begin{bmatrix} y_{1} \\\\ y_{2} \\end{bmatrix}$\n",
        "\n",
        "> $W = \\begin{bmatrix} w_{1} & w_{2} & w_{3} \\\\ w_{4} & w_{5} & w_{6} \\end{bmatrix}$\n",
        "\n",
        "> $x = \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ x_{3}\\end{bmatrix}$\n",
        "\n",
        "> $b = \\begin{bmatrix} b_{1} \\\\ b_{2} \\end{bmatrix}$\n",
        "\n",
        "If we assume that $x$ is our input and $y$ is our output, we can see that we have a non-linear expression with respect to $x$. Note that in the absence of an activation function, we will end up with a linear classifier.\n",
        "\n",
        "If we add a second layer to make our model more expressive, we will have:\n",
        "\n",
        "> $y^{(2)} = f(W^{(2)}z + b^{(2)})$\n",
        "\n",
        "where $z = W^{(1)}x + b^{(1)}$.\n",
        "\n",
        "This can be written in a more general format for current layer $L $and previous layer $L-1$ as:\n",
        "\n",
        "> $y^{L} = f(W^{L}.\\sigma(W^{L-1}x^{L-2} + b^{L-1}) + b^{L})$\n",
        "\n",
        "The inner activation is denoted with $\\sigma$, which is the symbol we use for non-linear activation functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mudb-I-faTf"
      },
      "source": [
        "# Universal approximation theorem\n",
        "\n",
        "According to the universal approximation theorem, given enough neurons andthe correct set of weights, a multi-layer NN can approximate any function.Learning this function is increasingly hard, and we have no guarantee that ourdata are enough to do so.\n",
        "\n",
        "Admittedly, that doesn’t mean we should only use NNs. \n",
        "\n",
        "In fact, we will learn about other models and how we can make a NN more compact, wider, or deeper to learn very rich data representations.\n",
        "\n",
        "Why is that even useful?\n",
        "\n",
        "Because NNs hide another secret besides being very good function approximators. They are also very good feature extractors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X32svJ_nfaTi"
      },
      "source": [
        "# Deep neural networks as feature extractors\n",
        "\n",
        "> Feature extraction can be seen as the transformation of the input data points from the input space to the feature space where classification is much easier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lN1dVTafaTl"
      },
      "source": [
        "Here is an intuitive and oversimplified example:\n",
        "\n",
        "Imagine that each data point has 70 dimensions. Finding the correct 70-dimensional function to distinguish the data into two categories is very difficult and time consuming. \n",
        "\n",
        "Instead, we transform our input to a three-dimensional space where a classifier can approximate the decision boundary more easily. If we transform the 3D decision boundary back to the 70-dimensional space, we will see that itcorresponds to a 70-dimensional decision boundary. \n",
        "\n",
        "The transformed space does not always need to be low-dimensional, but high-dimensional spaces do not guarantee better results either. Think of the 70-dim example: if one of these input dimensions refers to the label,it would be enough to have 100% accuracy.\n",
        "\n",
        "In any case, this is the main reason Deep Neural Networks (DNNs) exist: to transform the input data into a “better” space. Better because we can classify the (new) data more easily after we transform them!\n",
        "\n",
        "In fact, in most real-life applications, only the last one or two layers of a neural network performs the actual classification. The rest account for feature extraction and learning representations.\n",
        "\n",
        "\n",
        "![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig08.png)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "03. Neural_networks.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}