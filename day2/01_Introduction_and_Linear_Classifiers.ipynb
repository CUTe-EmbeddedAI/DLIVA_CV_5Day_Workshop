{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B_vPsw8Zdj0"
      },
      "source": [
        "# What is deep learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrxrVxWtZdkB"
      },
      "source": [
        "Deep learning is considered a subfield of machine learning. Even though thereare countless inspirations from real neurons, we will focus on modelingeverything with formulas, intuitions, and theories that actually work.\n",
        "\n",
        "In practice, deep learning is the scaling up of computational structures calledneural networks.\n",
        "\n",
        "Why do we take the time to develop such approaches?\n",
        "\n",
        "Because it is the optimal solution when working with really large-scale data rightnow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSCNDZTdZdkC"
      },
      "source": [
        "> It is important to keep in mind that deep learning is all about learning\n",
        "**powerful representations**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNiENfGdZdkD"
      },
      "source": [
        "There is a huge shift from extracting features to learning features, and that is what deep learning is all about."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UEn8gtcZdkE"
      },
      "source": [
        "# Deep learning applications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp6GsEy9ZdkF"
      },
      "source": [
        "In this course, you will get a general perspective of a huge variety of problems that you can solve with deep learning.\n",
        "\n",
        "First, you will learn to formulate problems in terms of machine and deep learning. That’s a crucial skill that you will use throughout your career and projects. We will be focussing our applications to computer vision related tasks.\n",
        "\n",
        "Secondly, you will learn the most basic components that tackle some of the following tasks:\n",
        "- Image classification\n",
        "- Image regression\n",
        "- Object detection\n",
        "- Generative models\n",
        "- Embedded deep learning\n",
        "\n",
        "Deep learning has already transformed a variety of businesses such as websearch, augmented reality, social networks, automobiles, retail, cybersecurity, and manufacturing. But the most exciting thing is the potential novel applications thatmay appear in the future. These projects can radically transform every industry.\n",
        "\n",
        "Some experts claim that **AI is the new electricity**.\n",
        "\n",
        "While this may be a disputable idea, what is for certain is that **deep learning is one of the most sought after and well-paid skills**.\n",
        "\n",
        "So why stay behind?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "encnxUtyZdkG"
      },
      "source": [
        "# Linear Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwdfdhPUZdkH"
      },
      "source": [
        "Explore linear classifiers, their principles, and their training process.\n",
        "\n",
        "We will cover the following:\n",
        "\n",
        "- What is a linear classifier?\n",
        "- Training a classifier\n",
        "- Loss function\n",
        "- Optimization and training process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z7ur842ZdkI"
      },
      "source": [
        "# What is a linear classifier?\n",
        "\n",
        "Suppose we want to build a machine learning model to classify the following points into two categories based on their color. It is very easy to see that we can find a single point that separates them perfectly. The goal of our model is to find this point.\n",
        "\n",
        "\n",
        "![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig01.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kZe829vZdkK"
      },
      "source": [
        "The easiest way to do that is to build a linear classifier. Our classifier has the form $f(x,w) = w_{1}x_{1} + w_{2}$. The purpose of\n",
        "$f(x, w)$ will be to find the parameters $w_{1}$ and $w_{2}$, so that any corresponding scalar point (1D) can be distinguished perfectly. If\n",
        "$f (x, w) > 0$, the point belongs to the blue category. Otherwise, it belongs to the red.\n",
        "\n",
        "Sounds easy?\n",
        "\n",
        "Let’s extend this idea to 2D data points!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJ7Qlsz1ZdkL"
      },
      "source": [
        "Each point will now be represented as $(x_{1} , x_{2})$.\n",
        "\n",
        "For the 2D case, we need to find a line (instead of a point) that separates our 2Dpoints, so our classifier will be $(x, w) = w_{1}x_{1} + w_{2}x_{2} + w_{3}$. Again, the classifier should be trained to find the optimal\n",
        "$w_{1} , w_{2}$ and $w_{3}$.\n",
        "\n",
        "\n",
        "![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig02.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Vj5fIgPZdkN"
      },
      "source": [
        "This idea can be naturally extended to arbitrary (N) dimensions. The line in the2D space will be a plane in the 3D space and N-plane in higher dimensions. We call this the hyperplane of the N-dimensional space that can separate the space into 2 classes (red and blue).\n",
        "\n",
        "We will utilize linear algebra and matrices to formulate it. To facilitate thereadability, matrices will be denoted in capital letter.\n",
        "\n",
        "As a result, we now have $f(x, W) = Wx + b$, where $x$ and $b$ are N-dimensional vectors while $W$ is an $N$ x $N$ matrix. We need to find the correct values of $W$ and $b$ to define a hyperplane. When we have those, we can receive the category $y$ for any data-point $x$.\n",
        "\n",
        "From now on, we’ll denote our classifier as $f(x_{i} ,W)$\n",
        "![linear](images/fig03.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-cG0-ekZdkO"
      },
      "source": [
        "In Pytorch, we can build a linear classifier with 5 inputs and 10 outputs using just one line of code. The following code will initialize a trainable matrix and a vector and every time we use class instance classifier, it will perform the operation\n",
        "$y = Wx + b$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlCVRAoQZdkO",
        "outputId": "881d954c-73a6-4c01-b0a6-28d545237f32"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=5, out_features=10, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "## Basic imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "## initializes a matrix W and a vector b\n",
        "classifier = nn.Linear(5, 10)\n",
        "\n",
        "classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOEmujQcZdkR"
      },
      "source": [
        "# Training a classifier\n",
        "\n",
        "We know that we want to find the matrix $W$ and the vector $b$ in order to classify our examples. \n",
        "\n",
        "But how? \n",
        "\n",
        "First of all, we need training data. Training data are data-points ($x$) whose category (target class $t$) we are aware of. Thus, we can utilize it to “train” our classifier.\n",
        "\n",
        "> “Training” a classifier refers to the notion of trying to find the matrix $W$ by feeding to its already known data points.\n",
        "\n",
        "Because we know the “labels” (category) of the data, these training approaches are called **supervised**. The data are provided in pairs $(x,t)$. We use the $x$ as an input to the classifier and the labels $t$ to compute the loss (distance). Note that $y$ refers to the output of the classifier and will be equal to $y = Wx + b$\n",
        "\n",
        "> Intuitively, we will push the randomly initialized model to learn this mapping from $x \\rightarrow t$\n",
        "\n",
        "Before we describe the process of training, we need to describe two more concepts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0kYH-ZPZdkS"
      },
      "source": [
        "# Loss function\n",
        "\n",
        "**Loss (or cost) is a measure of how good or bad a classification of a data-point is.** Alternatively, it can be defined as how far the classifier’s prediction $y$ is, for the data-point $x$, from the actual class $t$. Let’s make that crystal clear:\n",
        "\n",
        "Given a dataset $(x_{i} , t_{i})$ of N points where $x_{i}$ is an N-dimensional point in space and $t_{i}$ is an integer that defines the point’s category, loss is the distance between $f(x_{i} ,W)$ and $t_{i}$ .\n",
        "\n",
        "$C_{i}(f (x_{i},W), t_{i})$ is the cost for a single example $x_{i}$.\n",
        "\n",
        "The overall loss of the entire training data is simply the average of all the individual losses. However, in practice, we rarely average the loss over all datapoints.\n",
        "\n",
        "Note that the choice of the loss function depends on the problem and the form of the data. In our case, from now on, we will use the mean squared error distance defined as:\n",
        "\n",
        "> $C = \\sum(f (x _{i},W) − t_{i})^{2}$\n",
        "\n",
        "Notice that the sum is between the elements of the vector. Here is a code example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6l5RjIJZdkS",
        "outputId": "108093b2-b65c-42a1-eb16-7c0fe2f3f2d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction:  tensor([ 1.0859, -0.3607, -1.1807], grad_fn=<AddBackward0>)\n",
            "Output:  tensor(2.0215, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# define a linear model\n",
        "model = nn.Linear(10,3)\n",
        "\n",
        "# define loss fn\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "## dummy input x\n",
        "input_vector = torch.randn(10)\n",
        "## class number 3, denoted as a vector with the class index to 1\n",
        "target = torch.tensor([0,0,1])\n",
        "## y in math\n",
        "pred = model(input_vector)\n",
        "output = loss(pred, target)\n",
        "\n",
        "print(\"Prediction: \" ,pred)\n",
        "print(\"Output: \" , output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCIeF3M1ZdkT"
      },
      "source": [
        "It is important to understand that even though the target class is a scalar (3 in the example above), we convert it to a tensor. For three classes, you will have these possible target vectors $t$:\n",
        "\n",
        "class 1 $\\rightarrow$ [1,0,0]\n",
        "\n",
        "class 2 $\\rightarrow$ [0,1,0]\n",
        "\n",
        "class 3 $\\rightarrow$ [0,0,1]\n",
        "\n",
        "This is also called **one-hot encoding** in machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ebQJ7RjZdkT"
      },
      "source": [
        "# Optimization and training process\n",
        "\n",
        "Optimization is the process of finding the weight matrix $W$ that minimizes the loss function. In other words, it is the process of selecting the individual weights $w_{i}$ so that the classifier’s prediction $y$ is as close as possible to the point’s real label $t$.\n",
        "\n",
        "Mathematically this can be written as:\n",
        "\n",
        "> $w′ = argmin_{w}(C(w))$\n",
        "\n",
        "For now, let’s keep in mind that optimization is an abstract concept that describes how we select the matrix. We will dive into it in the next lesson where we will talk about neural networks.\n",
        "\n",
        "Now, we can describe the training algorithm in its entirety:\n",
        "\n",
        "Given a set of training examples $x_{i}$ with their labels $t_{i}$, we need to:\n",
        "\n",
        "- Initialize the classifier $f(x_{i},W)$ with random weight $W$\n",
        "\n",
        "- Feed a training example in the classifier and get the output $y$\n",
        "\n",
        "- Compute the loss between the prediction $y_{i}$ and target $t_{i}$.\n",
        "\n",
        "- Adjust the weights $W$ according to the loss $C_{i}$(next lesson).\n",
        "\n",
        "- Repeat for all training examples.\n",
        "\n",
        "This is the core idea behind all deep learning models. In the end, we will have a trained classifier that can be **generalized in previously UNSEEN examples**.\n",
        "\n",
        "> The only step that should be unclear now is how we adjust the weights. We will discuss this in the next lesson."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "01. Introduction_and_Linear_Classifiers.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}