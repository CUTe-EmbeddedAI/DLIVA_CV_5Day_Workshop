{"cells":[{"cell_type":"markdown","metadata":{"id":"iDfNIQIXAVXO"},"source":["## Convolution in Practice\n","\n","Find out why convolutional and pooling layers are the building blocks of Convolutional Neural Networks.\n","\n","We will be covering:\n","\n","- Spatial dimensions\n","\n","Pooling layers"]},{"cell_type":"markdown","metadata":{"id":"tVlHFdJCAVXZ"},"source":["When it comes to real-life applications, most images are in fact a 3D tensor with width, height, and 3 channels (R,G,B) as dimensions.\n","\n","In that case, the kernel should also be a 3D tensor (k $\\times$ k $\\times$ $channels$). Each kernel will produce a 2D feature map. Remember the sliding happens only across width and height. We just take the dot product of all the input channels on the computation. Each kernel will produce 1 output channel.\n","\n","In practice, we tend to use more than 1 kernel in order to capture different kinds of features at the same time.\n","\n","![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig17.png)"]},{"cell_type":"markdown","metadata":{"id":"tMRA_3OjAVXc"},"source":["As you may have guessed, our learnable weights are now the values of our filters and can be trained with backpropagation, as usual. We can add a bias into each filter as well.\n","\n","Convolutional layers can be stacked on top of others. Since convolutions are linear operators, we include non-linear activation functions in between just as we did in fully connected layers.\n","\n","To recap, you have to think in terms of input channels, output channels, and kernel size. And that is exactly how we are going to define it in Pytorch.\n","\n","To define a convolutional network in Pytorch, we have:"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xbzTq9-3AVXe","executionInfo":{"status":"ok","timestamp":1658109207193,"user_tz":-480,"elapsed":3919,"user":{"displayName":"احمد اسعد","userId":"15464471792373836994"}},"outputId":"effef970-155d-4fe2-919e-01c49b1250aa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Conv2d(3, 5, kernel_size=(5, 5), stride=(1, 1))\n"]}],"source":["import torch.nn as nn\n","\n","conv_layer = nn.Conv2d(in_channels=3, out_channels=5, kernel_size=5)\n","print(conv_layer)"]},{"cell_type":"markdown","metadata":{"id":"xXAssUyTAVXi"},"source":["The above layer will receive an image of 3 channels (i.e., R,G,B) and will output 5 feature maps (channels) using a kernel size of 5x5x3. For simplicity, we just say 5x5 kernel."]},{"cell_type":"markdown","metadata":{"id":"cqDjEf6TAVXj"},"source":["### Spatial dimensions"]},{"cell_type":"markdown","metadata":{"id":"5HI2iHmdAVXj"},"source":["Because figuring out the dimensions of each tensor can quickly become complicated, let’s examine a simple example of a convolutional layer. Assume that we have a 7x7 image and a 3x3 filter. The feature map will be of size 5x5.\n","\n","![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig18.PNG)\n","\n","This is the most common approach but it is not the only one.\n","\n","Sometimes, we may want to slide/move the kernel every 2 pixels instead of every 1, thus introducing an extra hyperparameter called **stride**.\n","\n","In some cases, we can also pad the image around the edges with zeros in order to control the output dimensions. The amount of zero-padding on the edges introduces another parameter in the mix called **zero-padding** or simply **padding**.\n","\n","If we introduce a stride of 2 and zero-padding of 1, we will receive an image of 4x4:\n","\n","![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig19.PNG)\n","\n","To summarize, given an input of size $W_1 \\times H_1 \\times D_1$, a number of output channels $K$ with kernel size $F \\times F$, stride $S$ and padding $P$, we acquire an output of size $W_2 \\times H_2 \\times D_2$ where:\n","\n","![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig20.PNG)\n","\n","The above example can be validated in 3 lines of Pytorch code.\n","\n","In the input tensor, 1 refers to the batch size. You can ignore it for now."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0fyQka_dAVXm","executionInfo":{"status":"ok","timestamp":1658109299227,"user_tz":-480,"elapsed":372,"user":{"displayName":"احمد اسعد","userId":"15464471792373836994"}},"outputId":"8891f9ef-8487-46c5-aabb-f9aa65a1b12b"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 6, 4, 4])\n"]}],"source":["import torch\n","import torch.nn as nn\n","\n","input_img = torch.rand(1,3,7,7)\n","layer = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=2, padding=1)\n","out = layer(input_img)\n","print(out.shape) "]},{"cell_type":"markdown","metadata":{"id":"cNJR70WZAVXn"},"source":["### Pooling layer\n","\n","Many popular CNN architectures utilize another type of layer besides the convolutional layer. This new layer is known as the pooling layer. Pooling layers can be thought of as a way to **downsample** the features without having any learnable parameters. In other words, pooling layers do not contribute to the training of a neural network. These layers function in a similar form as convolutional layers in terms that we apply a function on a chunk of the input and produce a single scalar number. The most common way is known as max-pooling and works as shown below:\n","\n","![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig21.PNG)\n","\n","From each rectangular 2x2 chunk of our image, we keep only the bigger element in our feature map, resulting in a tensor with half the size of our initial input. In other words, we collapse each non-overlapping chunk into a single value.\n","\n","One reason that we may introduce pooling is that it adds invariance to minor spatial changes. For example, two tensors with slightly different translations will result in the same pooling map.\n","\n","Another reason is that we want to gradually reduce the resolution of the input as we perform the forward pass. That’s because the deeper layers should have a higher receptive field, meaning that they should be more and more sensitive to the entire image.\n","\n","After all, our ultimate goal is to classify if an image contains a cat or a dog and not detect the corners.\n","\n","Finally, pooling makes the learned features more abstract."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DmdOwa_OAVXp","executionInfo":{"status":"ok","timestamp":1658109332418,"user_tz":-480,"elapsed":366,"user":{"displayName":"احمد اسعد","userId":"15464471792373836994"}},"outputId":"72a82b51-ed6d-49d4-c0fd-9badd7ee9de5"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 3, 4, 4])\n"]}],"source":["import torch\n","import torch.nn as nn\n","\n","input_img = torch.rand(1,3,8,8)\n","layer = nn.MaxPool2d(kernel_size=2, stride=2)\n","out = layer(input_img)\n","print(out.shape) "]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"08. Convolution_in_practice.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}