{"cells":[{"cell_type":"markdown","metadata":{"id":"HVihl7rPFO9W"},"source":["## Batch Normalization and Dropout\n","\n","Discover how batch normalization and dropout improve a model's accuracy.\n","\n","We will be covering:\n","\n","- Batch Normalization\n","\n","- Notations\n","\n","- Advantages and disadvantages of using batch normalization\n","\n","- Dropout\n","\n","### Batch Normalization\n","\n","If you open any introductory machine learning textbook, you will find the idea of **input scaling**. It is undesirable to train a model with **gradient descent** with non-normalized input features.\n","\n","Let’s start with an intuitive example to understand why we want normalization inside any model.\n","\n","Suppose you have an input feature $x1$ in the range [0,10000] and another feature $x2$ in the range [0,1]. Any linear combination would ignore $x2$ such that $x1*w1 + x2*w2 \\approx x1$, since our weights are initialized in a very tiny range like [-1,1].\n","\n","We encounter the same issues inside the layers of deep neural networks. In this lesson, we will propagate this idea inside the NN.\n","\n","> If we think out of the box, any intermediate layer is conceptually the same as the input layer; it accepts features and transforms them"]},{"cell_type":"markdown","metadata":{"id":"78X-xhlyFO9y"},"source":["### Notations\n","\n","Throughout this lesson, $N$ will be the batch size, $H$ will refer to the height, $W$ to the width, and $C$ to the feature channels. The greek letter $\\mu()$ refers to mean and the greek letter $\\sigma()$ refers to standard deviation.\n","\n","The batch features are denoted by $x$ with a shape of $[N, C, H, W]$.\n","\n","![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig22.PNG)\n","\n","We will visualize the 4D activation maps x by **merging the spatial dimensions**. Now, we have a 3D shape that looks like this:\n","\n","![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig23.PNG)\n","\n","The most dominant solution is batch normalization. Let’s see how it works.\n","\n","> Batch Normalization (BN) normalizes the mean and standard deviation **for each individual feature channel/map**.\n","\n","First of all, the mean and standard deviation are first-order statistics, and thus they relate to the **global characteristics** (such as the image style).\n","\n","In this way, we somehow blend the global characteristics. Such a strategy is effective when we want our representation to share these characteristics. This is the reason that we widely utilize BN in downstream tasks (i.e., image classification).\n","\n","From a mathematical point of view, **you can think of it as bringing the features of the image in the same range**.\n","\n","![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig24.PNG)\n","\n","Specifically, we demand from our features to follow a Gaussian distribution with zero mean and unit variance. Mathematically, this can be expressed as:\n","\n","![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig25.PNG)\n","\n","The index $c$ denotes the per-channel (feature map) mean.\n","\n","Let’s see this operation visually:\n","\n","![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig26.PNG)\n","\n","Notably, the spatial dimensions as well as the image batch are averaged. This way, **we concentrate our features in a compact Gaussian-like space**, which is usually beneficial.\n","\n","In fact, $\\gamma$ and $\\beta$ correspond to the trainable parameters that result in the linear/affine transformation, which is different for all channels.\n","\n","Specifically $\\gamma$ and $\\beta$ are vectors with the channel dimensionality."]},{"cell_type":"markdown","metadata":{"id":"Iq4OEKm3FO95"},"source":["### Advantages and disadvantages of using batch normalization\n","\n","The following are some **advantages** of BN:\n","\n","- BN accelerates the training of deep neural networks and tackles the vanishing gradient problem.\n","\n","- For every input mini-batch, we calculate different statistics. This introduces some sort of regularization. Regularization refers to any form of technique/constraint that restricts the complexity of a deep neural network during training.\n","\n","- BN also has a beneficial effect on the gradient flow through the network. It reduces the dependence of gradients on the scale of the parameters or of their initial values. This allows us to use much higher learning rates.\n","\n","- In theory, BN makes it possible to use saturating nonlinearities by preventing the network from getting stuck, but we just use nn.ReLU().\n","\n","- BN makes the gradients more predictive.\n","\n","BN has the following **disadvantages**:\n","\n","- Batch normalization may cause inaccurate estimation of batch statistics when we have a small batch size. This increases the model error. In tasks such as image segmentation, the batch size is usually too small. BN needs a sufficiently large batch size.\n","\n","Let’s now implement batch normalization from scratch for images of size $[N, C, H, W]$. All you have to do is transform the above equations to Pytorch. The tricky part is to correctly figure out the sizes of each tensor."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"c9RMg8gtFO9-","executionInfo":{"status":"ok","timestamp":1658110582415,"user_tz":-480,"elapsed":2751,"user":{"displayName":"احمد اسعد","userId":"15464471792373836994"}}},"outputs":[],"source":["import torch\n","\n","# Gamma and beta are provided as 1d tensors. \n","# X is the data in a mini-batch\n","\n","def batchnorm(X, gamma, beta):\n","\n","    # extract the dimensions\n","    N, C, H, W = list(X.size())\n","    # mini-batch mean\n","    mean = torch.mean(X, dim=(0, 2, 3))\n","    # mini-batch variance\n","    variance = torch.mean((X - mean.reshape((1, C, 1, 1))) ** 2, dim=(0, 2, 3))\n","    # normalize\n","    X_hat = (X - mean.reshape((1, C, 1, 1))) * 1.0 / torch.sqrt(variance.reshape((1, C, 1, 1)) )\n","    # scale and shift\n","    out = gamma.reshape((1, C, 1, 1)) * X_hat + beta.reshape((1, C, 1, 1))\n","\n","    return out  "]},{"cell_type":"markdown","metadata":{"id":"OKa3BdDeFO-F"},"source":["### Dropout\n","\n","Another technique to train deep learning models is dropout.\n","\n","Conceptually, dropout approximates training a large number of neural networks with different architectures in parallel.\n","\n","> In practice, during training, some number of layer outputs are randomly ignored (dropped out) with probability $p$. \n","\n","Thus, the same layer will alter its connectivity and will search for alternative paths to convey the information in the next layer. As a result, each update to a layer during training is performed with a different “view” of the configured layer.\n","\n","“Dropping” values means temporarily removing them from the network for the current forward pass along with all its incoming and outgoing connections.\n","\n","Dropout has the effect of making the training process noisy. The choice of the probability $p$ depends on the architecture.\n","\n","![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig27.PNG)\n","This conceptualization suggests that perhaps dropout breaks-up situations where network layers co-adapt to correct mistakes from prior layers, in turn making the model more robust.\n","\n","The neural network will adapt in a way that prevents overfitting, which refers to poor generalization to unseen data.\n","\n","Dropout increases the sparsity of the network and in general encourages sparse representations!\n","\n","You can find an example in the code below .\n","\n","Notice that each value will be zeroed with a probability of p=0.5. Nonetheless, that doesn’t imply that the output will be 50% zeroed-out every time."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TkEyw3pDFO-I","executionInfo":{"status":"ok","timestamp":1658110607192,"user_tz":-480,"elapsed":374,"user":{"displayName":"احمد اسعد","userId":"15464471792373836994"}},"outputId":"9b90c55b-e8a1-4767-a897-b91c1bd744a2"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.6593, 0.3519, 0.6087, 0.8115, 0.4393, 0.6048, 0.5635, 0.0887]])\n","tensor([[0.0000, 0.0000, 1.2174, 1.6230, 0.8786, 0.0000, 1.1270, 0.0000]])\n","tensor([[1.3187, 0.7039, 1.2174, 0.0000, 0.0000, 1.2096, 1.1270, 0.0000]])\n"]}],"source":["import torch\n","import torch.nn as nn\n","\n","inp = torch.rand(1,8)\n","layer = nn.Dropout(0.5)\n","out1 = layer(inp)\n","out2 = layer(inp)\n","print(inp)\n","print(out1)\n","print(out2)"]},{"cell_type":"markdown","metadata":{"id":"G2vah6ZTFO-L"},"source":["### Training with batchnorm\n","\n","Now, it's the time to apply batchnorm and dropout! Let's copy and paste the code for training a CNN in our previous notebook. \n","\n","Let's make the necessary imports and data preparation as we did before."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"OEyMWseGFO-O","executionInfo":{"status":"ok","timestamp":1658110612508,"user_tz":-480,"elapsed":1404,"user":{"displayName":"احمد اسعد","userId":"15464471792373836994"}}},"outputs":[],"source":["import os\n","\n","#Numpy is linear algebra lbrary\n","import numpy as np\n","# Matplotlib is a visualizations library \n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":136,"referenced_widgets":["adba8204f83d4273b1984b18a734b498","7be6b40bef92493bb44bbcafa83b0a1b","e20b525e52174632995c28404c7228b4","634ebc8b98f84bdba14057b19307dd01","3e92a10dc8c24a0cb02e7fe075d5c48d","090e59acd4f8421eba9e81a1e9a638d9","7b74cbd75d8b4662ac7544b50188502b","bc9a4934b694476aa59af65886850132","896dace21ba449c8afcb8a74b73d08ac","9a36900454bf4b19ad869ffd12391fc3","d42e48f27f7b4aee9fb6787f442bc001"]},"id":"qBfYglSWFO-R","executionInfo":{"status":"ok","timestamp":1658110623022,"user_tz":-480,"elapsed":7633,"user":{"displayName":"احمد اسعد","userId":"15464471792373836994"}},"outputId":"a95b668f-e871-42c9-ac80-4daa1b0f2a06"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adba8204f83d4273b1984b18a734b498"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n","50000\n","10000\n"]}],"source":["transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","batch_size = 4\n","\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n","                                          shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n","                                         shuffle=False, num_workers=2)\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","       'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","train_data_size = len(trainloader.dataset)\n","test_data_size = len(testloader.dataset)\n","\n","print(train_data_size)\n","print(test_data_size)"]},{"cell_type":"markdown","metadata":{"id":"PhztnMavFO-X"},"source":["Here, try to incorporate batchnorm as a layer in this vanilla CNN.\n","\n","Note that in pytorch, batchnorm can be implemented using `nn.BatchNorm2d(num_features)` for 2D input and `nn.BatchNorm2d(num_features)` for 1D input. "]},{"cell_type":"code","execution_count":5,"metadata":{"id":"aocAQZf_FO-Z","executionInfo":{"status":"ok","timestamp":1658110624986,"user_tz":-480,"elapsed":543,"user":{"displayName":"احمد اسعد","userId":"15464471792373836994"}}},"outputs":[],"source":["#1. DEFINE THE CNN WITH BATCHNORM\n","class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 6, 5)\n","        self.pool = nn.MaxPool2d(2, 2)\n","#         self.batchnorm = nn.BatchNorm2d(6),\n","#         self.batchnorm = batchnorm()\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","        self.relu = nn.ReLU()\n","        # DEFINE BATCHNORM LAYER HERE #\n","\n","    def forward(self, x):\n","        x = self.pool(self.relu(self.conv1(x)))\n","        x = self.pool(self.relu(self.conv2(x)))\n","        # INCLUDE BATCHNORM IN THE FORWARD METHOD #\n","        x = x.view(-1, 16 * 5 * 5)\n","        x = self.relu(self.fc1(x))\n","        x = self.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vgWT1BGZFO-b","executionInfo":{"status":"ok","timestamp":1658110627325,"user_tz":-480,"elapsed":8,"user":{"displayName":"احمد اسعد","userId":"15464471792373836994"}},"outputId":"b54ea4bb-18a2-4c2f-8df1-2787a8ef85fb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["CNN(\n","  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n","  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n","  (fc1): Linear(in_features=400, out_features=120, bias=True)\n","  (fc2): Linear(in_features=120, out_features=84, bias=True)\n","  (fc3): Linear(in_features=84, out_features=10, bias=True)\n","  (relu): ReLU()\n",")"]},"metadata":{},"execution_count":6}],"source":["model = CNN() # need to instantiate the network to be used in instance method\n","\n","# 2. LOSS AND OPTIMIZER\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","\n","# 3. move the model to GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","model.to(device)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"iqUb-WREFO-e","executionInfo":{"status":"ok","timestamp":1658110629460,"user_tz":-480,"elapsed":560,"user":{"displayName":"احمد اسعد","userId":"15464471792373836994"}}},"outputs":[],"source":["import time # to calculate training time\n","\n","def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n","    '''\n","    Function to train and validate\n","    Parameters\n","        :param model: Model to train and validate\n","        :param loss_criterion: Loss Criterion to minimize\n","        :param optimizer: Optimizer for computing gradients\n","        :param epochs: Number of epochs (default=25)\n","  \n","    Returns\n","        model: Trained Model with best validation accuracy\n","        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n","    '''\n","    \n","    start = time.time()\n","    history = []\n","    best_acc = 0.0\n","\n","    for epoch in range(epochs):\n","        epoch_start = time.time()\n","        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n","        \n","        # Set to training mode\n","        model.train()\n","        \n","        # Loss and Accuracy within the epoch\n","        train_loss = 0.0\n","        train_acc = 0.0\n","        \n","        valid_loss = 0.0\n","        valid_acc = 0.0\n","        \n","        for i, (inputs, labels) in enumerate(trainloader):\n","\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            \n","            # Clean existing gradients\n","            optimizer.zero_grad()\n","            \n","            # Forward pass - compute outputs on input data using the model\n","            outputs = model(inputs)\n","            \n","            # Compute loss\n","            loss = loss_criterion(outputs, labels)\n","            \n","            # Backpropagate the gradients\n","            loss.backward()\n","            \n","            # Update the parameters\n","            optimizer.step()\n","            \n","            # Compute the total loss for the batch and add it to train_loss\n","            train_loss += loss.item() * inputs.size(0)\n","            \n","            # Compute the accuracy\n","            ret, predictions = torch.max(outputs.data, 1)\n","            correct_counts = predictions.eq(labels.data.view_as(predictions))\n","            \n","            # Convert correct_counts to float and then compute the mean\n","            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n","            \n","            # Compute total accuracy in the whole batch and add to train_acc\n","            train_acc += acc.item() * inputs.size(0)\n","            \n","            #print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n","\n","            \n","        # Validation - No gradient tracking needed\n","        with torch.no_grad():\n","\n","            # Set to evaluation mode\n","            model.eval()\n","\n","            # Validation loop\n","            for j, (inputs, labels) in enumerate(testloader):\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # Forward pass - compute outputs on input data using the model\n","                outputs = model(inputs)\n","\n","                # Compute loss\n","                loss = loss_criterion(outputs, labels)\n","\n","                # Compute the total loss for the batch and add it to valid_loss\n","                valid_loss += loss.item() * inputs.size(0)\n","\n","                # Calculate validation accuracy\n","                ret, predictions = torch.max(outputs.data, 1)\n","                correct_counts = predictions.eq(labels.data.view_as(predictions))\n","\n","                # Convert correct_counts to float and then compute the mean\n","                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n","\n","                # Compute total accuracy in the whole batch and add to valid_acc\n","                valid_acc += acc.item() * inputs.size(0)\n","\n","                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n","            \n","        # Find average training loss and training accuracy\n","        avg_train_loss = train_loss/train_data_size \n","        avg_train_acc = train_acc/train_data_size\n","\n","        # Find average training loss and training accuracy\n","        avg_test_loss = valid_loss/test_data_size \n","        avg_test_acc = valid_acc/test_data_size\n","\n","        history.append([avg_train_loss, avg_test_loss, avg_train_acc, avg_test_acc])\n","                \n","        epoch_end = time.time()\n","    \n","        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_test_loss, avg_test_acc*100, epoch_end-epoch_start))\n","        \n","        # Save if the model has best accuracy till now\n","        torch.save(model, 'cifar10_model_'+str(epoch)+'.pt')\n","            \n","    return model, history"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ngaHvvWZFO-i","outputId":"9e38d96d-6310-4ed9-b630-949f147089bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/10\n","Epoch : 000, Training: Loss: 1.6285, Accuracy: 40.0720%, \n","\t\tValidation : Loss : 1.4729, Accuracy: 45.7100%, Time: 78.6272s\n","Epoch: 2/10\n"]}],"source":["# 4. Train the model for 10 epochs\n","\n","num_epochs = 10\n","trained_model, history = train_and_validate(model, criterion, optimizer, num_epochs)"]},{"cell_type":"markdown","metadata":{"id":"DLDK3OpMFO-k"},"source":["### Training with dropout\n","\n","Adding dropout to your PyTorch models is very straightforward with the `torch.nn.Dropout` class, which takes in the dropout rate – the probability of a neuron being deactivated – as a parameter.\n","\n","Go back to the CNN class and try to incorporate dropout as an additional layer in the fully connected layer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ksfu_3QoFO-m"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3.7 (pytorch_hasan)","language":"python","name":"torch"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"colab":{"name":"10. Batch_Normalization_and_Dropout.ipynb","provenance":[],"collapsed_sections":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"adba8204f83d4273b1984b18a734b498":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7be6b40bef92493bb44bbcafa83b0a1b","IPY_MODEL_e20b525e52174632995c28404c7228b4","IPY_MODEL_634ebc8b98f84bdba14057b19307dd01"],"layout":"IPY_MODEL_3e92a10dc8c24a0cb02e7fe075d5c48d"}},"7be6b40bef92493bb44bbcafa83b0a1b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_090e59acd4f8421eba9e81a1e9a638d9","placeholder":"​","style":"IPY_MODEL_7b74cbd75d8b4662ac7544b50188502b","value":"100%"}},"e20b525e52174632995c28404c7228b4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc9a4934b694476aa59af65886850132","max":170498071,"min":0,"orientation":"horizontal","style":"IPY_MODEL_896dace21ba449c8afcb8a74b73d08ac","value":170498071}},"634ebc8b98f84bdba14057b19307dd01":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9a36900454bf4b19ad869ffd12391fc3","placeholder":"​","style":"IPY_MODEL_d42e48f27f7b4aee9fb6787f442bc001","value":" 170498071/170498071 [00:02&lt;00:00, 63140620.53it/s]"}},"3e92a10dc8c24a0cb02e7fe075d5c48d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"090e59acd4f8421eba9e81a1e9a638d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b74cbd75d8b4662ac7544b50188502b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bc9a4934b694476aa59af65886850132":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"896dace21ba449c8afcb8a74b73d08ac":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9a36900454bf4b19ad869ffd12391fc3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d42e48f27f7b4aee9fb6787f442bc001":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}