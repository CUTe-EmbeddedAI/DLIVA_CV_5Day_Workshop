{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO\n",
    "\n",
    "YOLO is one of the most famous object detection algorithms available. It only needs few samples for training, while providing faster training times and high accuracy. We will demonstrate these features one-by-one in this wiki, while explaining the complete machine learning pipeline step-by-step where you collect data, label them, train them and finally detect objects using the trained data by running the trained model on an edge device such as the NVIDIA Jetson platform. Also, we will compare the difference between using custom datasets and public datasets.\n",
    "\n",
    "### What is YOLOv5?\n",
    "\n",
    "YOLO is an abbreviation for the term ‘You Only Look Once’. It is an algorithm that detects and recognizes various objects in an image in real-time. Ultralytics YOLOv5 is the latest version of YOLO and it is now based on the PyTorch framework.\n",
    "\n",
    "<div> <img src=\"images/fig45.png\" alt=\"Drawing\" style=\"width: 500px;\"/></div> \n",
    "\n",
    "### What is few-shot object detection?\n",
    "\n",
    "Traditionally if you want to train a machine learning model, you would use a public dataset such as the Pascal VOC 2012 dataset which consists of around 17112 images. However, we will use transfer learning to realize few-shot object detection with YOLOv5 which needs only a very few training samples. \n",
    "\n",
    "### Getting started\n",
    "Running your first object detection project on an edge device such as the Jetson platform simply involves 4 main steps!\n",
    "\n",
    "1. Collect dataset or use publically available dataset\n",
    "\n",
    "\n",
    "2. Collect dataset manually\n",
    "- Use publicly available dataset\n",
    "- Annotate dataset using Roboflow\n",
    "\n",
    "\n",
    "3. Train on local PC or cloud\n",
    "- Train on local PC (Linux)\n",
    "- Train on Google Colab\n",
    "\n",
    "4. Inference on Jetson device\n",
    "\n",
    "### Collect dataset or use publically available dataset\n",
    "\n",
    "The very first step of an object detection project is to obtain data for training. You can either download datasets available publicly or create your own dataset! Usually public datasets are used for education and research purposes. However, if you want to build specific object detection projects where the public datasets do not have the objects that you want to detect, you might want to build your own dataset.\n",
    "\n",
    "### Collect dataset manually\n",
    "\n",
    "It is recommended that you first record a video footage of the object that you want to recognize. You have to make sure that you cover all angles (360 degrees) of the object, place the object in different environments, different lighting and different weather conditions. The total video we recorded is 9 minutes long where 4.5 minutes is for flowers and the remaining 4.5 minutes is for leaves. The recording can be broken down as follows:\n",
    "\n",
    "<div> <img src=\"images/fig46.gif\" alt=\"Drawing\" style=\"width: 500px;\"/></div> \n",
    "\n",
    "1. morning normal weather\n",
    "2. morning windy weather\n",
    "3. Morning rainy weather\n",
    "4. Noon normal weather\n",
    "5. Noon windy weather\n",
    "6. Noon rainy weather\n",
    "7. Evening normal weather\n",
    "8. Evening windy weather\n",
    "9. Evening rainy weather\n",
    "\n",
    "**Note:** Later on, we will convert this video footage into a series of images to make up the dataset for training.\n",
    "\n",
    "### Use publicly available dataset\n",
    "\n",
    "You can also download a number of publically available datasets such as the COCO dataset, Pascal VOC dataset and much more. You can simply search open-source datasets on Google and choose from a variety of datasets available.\n",
    "\n",
    "### Annotate dataset using Roboflow\n",
    "\n",
    "Next we will move on to annotating the dataset that we have. Annotating means simply drawing rectangular boxes around each object that we want to detect and assign them labels. We will explain how to do this using Roboflow.\n",
    "\n",
    "[Roboflow](https://roboflow.com/) is an annotation tool based online. Here we can directly import the video footage that we recorded before into Roboflow and it will be exported into a series of images. This tool is very convenient because it will let us help distribute the dataset into \"training, validation and testing\". Also this tool will allow us to add further processing to these images after labelling them. Furthermore, it can easily export the labelled dataset into **YOLOV5 PyTorch format** which is what we exactly need!\n",
    "\n",
    "**Step 1.** Click here to sign up for a Roboflow account\n",
    "\n",
    "**Step 2.** Click Create New Project to start our project\n",
    "\n",
    "<div> <img src=\"images/fig47.png\" alt=\"Drawing\" style=\"width: 500px;\"/></div> \n",
    "\n",
    "**Step 3.** Fill in **Project Name**, keep the **License (CC BY 4.0)** and **Project type (Object Detection (Bounding Box))** as default. Under **What will your model predict?** column, fill in an annotation group name. For example, in our case we choose **plants**. This name should highlight all of the classes of your dataset. Finally, click **Create Public Project**.\n",
    "\n",
    "<div> <img src=\"images/fig48.png\" alt=\"Drawing\" style=\"width: 500px;\"/></div> \n",
    "\n",
    "**Step 4.** Drag and drop the video footage that you recorded before\n",
    "\n",
    "<div> <img src=\"images/fig49.png\" alt=\"Drawing\" style=\"width: 500px;\"/></div> \n",
    "\n",
    "**Step 5.** Choose a framerate so that the video will be divided into a series of images. Here we will use the default frame rate which is **1 frame/second** and this will generate 542 images in total. Once you select a frame rate by scrubbing through the slider, click **Choose Frame Rate**. It will take a few seconds to a few minutes (depending on the video length) to finish this process.\n",
    "\n",
    "**Step 6.** After the images are processed, click **Finish Uploading**. Wait patiently until the images are uploaded.\n",
    "\n",
    "<div> <img src=\"images/fig50.png\" alt=\"Drawing\" style=\"width: 500px;\"/></div> \n",
    "\n",
    "**Step 7.** After the images are uploaded, click **Assign Images**\n",
    "\n",
    "<div> <img src=\"images/fig51.png\" alt=\"Drawing\" style=\"width: 500px;\"/></div> \n",
    "\n",
    "**Step 8.** Select an image, draw a rectangular box around a flower, choose the label as pink flower and press ENTER\n",
    "\n",
    "<div> <img src=\"images/fig52.png\" alt=\"Drawing\" style=\"width: 500px;\"/></div> \n",
    "\n",
    "**Step 9.** Repeat the same for the remaining flowers\n",
    "\n",
    "**Step 10.** Draw a rectangular box around a leaf, choose the label as leaf and press ENTER\n",
    "\n",
    "**Step 11.** Repeat the same for the remaining leaves\n",
    "\n",
    "**Note:** Try to label all the objects that you see inside the image. If only a part of the object is visible, try to label that too.\n",
    "\n",
    "**Step 12.** Continue to annotate all the images in the dataset\n",
    "\n",
    "Roboflow has a feature called **Label Assist** where it can predict the labels beforehand so that your labelling will be much faster. However, it will not work with all object types, but rather a selected type of objects. To turn this feature on, you simply need to press the **Label Assist** button, **select a model, select the classes** and navigate through the images to see the predicted labels with bounding boxes.\n",
    "\n",
    "**Step 13.** Once labelling is done, click **Add images to Dataset**\n",
    "\n",
    "**Step 14.** Next we will split the images between \"Train, Valid and Test\". Keep the default percentages for the distribution and click **Add Images**\n",
    "\n",
    "<div> <img src=\"images/fig53.png\" alt=\"Drawing\" style=\"width: 500px;\"/></div> \n",
    "\n",
    "**Step 15.** Click **Generate New Version**\n",
    "\n",
    "**Step 16.** Now you can add Preprocessing and Augmentation if you prefer. \n",
    "\n",
    "**Step 17.** Next, proceed with the remaining defaults and click **Generate**\n",
    "\n",
    "**Step 18.** Click **Export**\n",
    "\n",
    "**Step 19.** Select **download zip to computer**, under \"Select a Format\" choose **YOLO v5 PyTorch** and click Continue\n",
    "\n",
    "**Step 20.** After that, a .zip file will be downloaded to your computer. We will need this .zip file later for our training.\n",
    "\n",
    "## Train on local PC or cloud\n",
    "\n",
    "After we are done with annotating the dataset, we need to train the dataset. For training we will introduce two methods. One method will be based online (Google Colab) and the other method will be based on local PC.\n",
    "\n",
    "For the Google Colab training, we will use two methods. In the first method, we will use Ultralytics HUB to upload the dataset, setup training on Colab, monitor the training and grab the trained model. In the second method, we will grab the dataset from Roboflow via Roboflow api, train and download the model from Colab.\n",
    "\n",
    "### Use Google Colab with Ultralytics HUB\n",
    "\n",
    "[Ultralytics HUB](https://hub.ultralytics.com/) is a platform where you can train your models without having to know a single line of code. Simply upload your data to Ultralytics HUB, train your model and deploy it into the real world! It is fast, simple and easy to use. Anyone can get started!\n",
    "\n",
    "Step 1. Visit [this link](https://hub.ultralytics.com/) to sign up for a free Ultralytics HUB account\n",
    "\n",
    "Step 2. Enter your credentials and sign up with email or sign up directly with a Google, GitHub or Apple account\n",
    "\n",
    "After you login to Ultralytics HUB, you will see the dashboard as follows\n",
    "\n",
    "<div> <img src=\"images/fig54.png\" alt=\"Drawing\" style=\"width: 700px;\"/></div> \n",
    "\n",
    "Step 3. Extract the zip file that we downloaded before from Roboflow and put all the included files inside a new folder\n",
    "\n",
    "Step 4. Make sure your dataset yaml and root folder (the folder we created before) share the same name. For example, if you name your yaml file as pinkflowers.yaml, the root folder should be named as pinkflowers.\n",
    "\n",
    "Step 5. Open pinkflowers.yaml file and edit train and val directories as follows\n",
    "\n",
    "<div> <img src=\"images/fig55.png\" alt=\"Drawing\" style=\"width: 700px;\"/></div> \n",
    "\n",
    "Step 6. Compress the root folder as a .zip and name it the same as the root folder (pinkflowers.zip in this example)\n",
    "\n",
    "Now we have prepared the dataset which is ready to be uploaded to Ultalytics HUB.\n",
    "\n",
    "Step 7. Click on the Datasets tab and click Upload Dataset\n",
    "\n",
    "<div> <img src=\"images/fig56.png\" alt=\"Drawing\" style=\"width: 700px;\"/></div> \n",
    "\n",
    "Step 8. Enter a Name for the dataset, enter a Description if needed, drag and drop the .zip file that we created before under Dataset field and click Upload Dataset\n",
    "\n",
    "Step 9. After the dataset is uploaded, click on the dataset to view more insights into the dataset\n",
    "\n",
    "<div> <img src=\"images/fig57.png\" alt=\"Drawing\" style=\"width: 700px;\"/></div> \n",
    "\n",
    "Step 10. Click on the Projects tab and click Create Project\n",
    "\n",
    "Step 11. Enter a Name for the project, enter a Description if needed, add a cover image if needed, and click Create Project\n",
    "\n",
    "Step 12. Enter the newly created project and click Create Model\n",
    "\n",
    "Step 13. Enter a Model name, choose YOLOv5n as the pretrained model, and click Next\n",
    "\n",
    "Note: Usually YOLOv5n6 is preferred as the pretrained model because it is suitable to be used for edge devices such as the Jetson platform. However, Ultralytics HUB still does not have the support for it. So we use YOLOv5n which is a slightly similar model.\n",
    "\n",
    "Step 14. Choose the dataset that we uploaded before and click Next\n",
    "\n",
    "Step 15. Choose Google Colab as the training platform and click the Advanced Options drop-down menu. Here we can change some settings for training. For example, we will change the number of epochs from 300 to 100 and keep the other settings as they are. Click Save\n",
    "\n",
    "<div> <img src=\"images/fig59.png\" alt=\"Drawing\" style=\"width: 700px;\"/></div> \n",
    "\n",
    "Note: You can also choose Bring your own agent if you are planning to perform local training\n",
    "\n",
    "Step 16. Copy the API key and click Open Colab\n",
    "\n",
    "Step 17. Replace MODEL_KEY with the API key that we copied before\n",
    "\n",
    "<div> <img src=\"images/fig60.png\" alt=\"Drawing\" style=\"width: 700px;\"/></div> \n",
    "\n",
    "Step 18. Click Runtime > Run All to run all the code cells and start the training process\n",
    "\n",
    "<div> <img src=\"images/fig61.png\" alt=\"Drawing\" style=\"width: 700px;\"/></div> \n",
    "\n",
    "Step 19. Come back to Ultralytics HUB and click Done when it turns blue. You will also see that Colab shows as Connected.\n",
    "\n",
    "<div> <img src=\"images/fig62.png\" alt=\"Drawing\" style=\"width: 700px;\"/></div> \n",
    "\n",
    "Now you will see the training progress on the HUB\n",
    "\n",
    "<div> <img src=\"images/fig63.png\" alt=\"Drawing\" style=\"width: 700px;\"/></div> \n",
    "\n",
    "Step 20. After the training is finished, click PyTorch to download the trained model in PyTorch format. PyTorch is the format that we need in order to perform inference on the Jetson device\n",
    "\n",
    "Note: You can also export into other formats as well which are displayed under Formats\n",
    "\n",
    "If you go back to Google Colab, you can see more details as follows:\n",
    "\n",
    "<div> <img src=\"images/fig64.png\" alt=\"Drawing\" style=\"width: 700px;\"/></div> \n",
    "\n",
    "Here the accuracy (mAP@.5) is about 90% and 99.4% for leaf and flower respectively, while the total accuracy (mAP@.5) is about 94.7%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (pytorch_hasan)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
